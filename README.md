# Build_BERT_basic
step 1: Pretrain 
Bidirectional Encoder Representations from Transformers (BERT)
  From Context-Independent to Context-Sensitive
  Languagemodel augmented sequence taggers (TagLM)
  CoVe (Context Vectors)
  ELMo (Embeddings from Language Models).
From Task-Specific to Task-Agnostic
BERT: Combining the Best of Both Worlds
Input Representation
Pretraining Tasks
  Masked Language Modeling
  Next Sentence Prediction
The Dataset for Pretraining BERT
Pretraining BERT
