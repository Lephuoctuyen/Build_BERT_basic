# Build_BERT_basic
Step 1: Pretrain 

Bidirectional Encoder Representations from Transformers (BERT)

  - From Context-Independent to Context-Sensitive
  
  - Languagemodel augmented sequence taggers (TagLM)
  
  - CoVe (Context Vectors)
  
  - ELMo (Embeddings from Language Models).
  
From Task-Specific to Task-Agnostic

BERT: Combining the Best of Both Worlds

Input Representation

Pretraining Tasks

  - Masked Language Modeling
    
  - Next Sentence Prediction

The Dataset for Pretraining BERT

Pretraining BERT

Step 2: Applications
Fine-Tuning BERT for Sequence-Level and Token-Level Applications

  - Single Text Classification
  - Text Pair Classification or Regression
  - Text Tagging
  - Question Answering
  - Summarize Text
